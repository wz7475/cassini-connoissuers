{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T13:27:01.986661Z",
     "start_time": "2024-09-14T13:27:01.078493Z"
    }
   },
   "source": [
    "import getpass\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from ipyleaflet import GeoJSON, Map, basemaps\n",
    "from sentinelhub import (\n",
    "    CRS,\n",
    "    BBox,\n",
    "    DataCollection,\n",
    "    MimeType,\n",
    "    SentinelHubDownloadClient,\n",
    "    SentinelHubRequest,\n",
    "    SHConfig,\n",
    ")\n",
    "import xarray as xr\n",
    "import rioxarray  # This registers the 'rasterio' engine\n",
    "# from sklearn.metrics import accuracy_score\n",
    "config = SHConfig()\n",
    "config.sh_client_id = \"sh-0470ee77-ae47-4290-a69b-1dd95df154b8\"\n",
    "config.sh_client_secret = \"IoKB7Ia7iIQM7AnoYm4H4H8aIWNwST4J\"\n",
    "config.sh_token_url = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "config.sh_base_url = \"https://sh.dataspace.copernicus.eu\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T14:52:35.336463Z",
     "start_time": "2024-09-14T14:52:35.327355Z"
    }
   },
   "source": [
    "evalscript_cloudless = \"\"\"\n",
    "//VERSION=3\n",
    "function setup() {\n",
    "    return {\n",
    "        input: [\"B08\", \"B04\", \"B03\", \"B02\", \"B11\", \"SCL\"],\n",
    "        output: {\n",
    "            bands: 4,\n",
    "            sampleType: \"INT16\"\n",
    "        },\n",
    "        mosaicking: \"ORBIT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "function getFirstQuartileValue(values) {\n",
    "    values.sort((a,b) => a-b);\n",
    "    return getFirstQuartile(values);\n",
    "}\n",
    "\n",
    "function getFirstQuartile(sortedValues) {\n",
    "    var index = Math.floor(sortedValues.length / 4);\n",
    "    return sortedValues[index];\n",
    "}\n",
    "\n",
    "function validate(sample) {\n",
    "    // Define codes as invalid:\n",
    "    const invalid = [\n",
    "        0, // NO_DATA\n",
    "        1, // SATURATED_DEFECTIVE\n",
    "        3, // CLOUD_SHADOW\n",
    "        7, // CLOUD_LOW_PROBA\n",
    "        8, // CLOUD_MEDIUM_PROBA\n",
    "        9, // CLOUD_HIGH_PROBA\n",
    "        10 // THIN_CIRRUS\n",
    "    ]\n",
    "    return !invalid.includes(sample.SCL)\n",
    "}\n",
    "\n",
    "function evaluatePixel(samples) {\n",
    "    var valid = samples.filter(validate);\n",
    "    if (valid.length > 0 ) {\n",
    "        let cloudless = {\n",
    "            b08: getFirstQuartileValue(valid.map(s => s.B08)),\n",
    "            b04: getFirstQuartileValue(valid.map(s => s.B04)),\n",
    "            b03: getFirstQuartileValue(valid.map(s => s.B03)),\n",
    "            b02: getFirstQuartileValue(valid.map(s => s.B02)),\n",
    "            b11: getFirstQuartileValue(valid.map(s => s.B11)),\n",
    "        }\n",
    "        let ndvi = ((cloudless.b08 - cloudless.b04) / (cloudless.b08 + cloudless.b04))\n",
    "        let moisture_idx = cloudless.b11\n",
    "        // This applies a scale factor so the data can be saved as an int\n",
    "        let scale = [cloudless.b04, cloudless.b03, cloudless.b02, ndvi, moisture_idx].map(v => v*10000);\n",
    "        return scale\n",
    "    }\n",
    "    // If there isn't enough data, return NODATA\n",
    "    return [-32768, -32768, -32768, -32768]\n",
    "}\n",
    "\"\"\"\n",
    "def interval_of_interest(year):\n",
    "    return (datetime(year, 6, 1), datetime(year, 9, 1))\n",
    "\n",
    "\n",
    "def get_request(year, coors):\n",
    "    time_interval = interval_of_interest(year)\n",
    "    epsg = 3035\n",
    "    bbox = BBox(coors, CRS(4326)).transform(epsg)\n",
    "    return SentinelHubRequest(\n",
    "        evalscript=evalscript_cloudless,\n",
    "        input_data=[\n",
    "            SentinelHubRequest.input_data(\n",
    "                data_collection=DataCollection.SENTINEL2_L2A.define_from(\n",
    "                    \"s2\", service_url=config.sh_base_url\n",
    "                ),\n",
    "                time_interval=time_interval,\n",
    "            )\n",
    "        ],\n",
    "        responses=[SentinelHubRequest.output_response(\"default\", MimeType.TIFF)],\n",
    "        bbox=bbox,\n",
    "        resolution=(10, 10),\n",
    "        config=config,\n",
    "        data_folder=\"./data\",\n",
    "    )\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T14:53:13.214957Z",
     "start_time": "2024-09-14T14:52:39.798198Z"
    }
   },
   "source": [
    "sh_requests = {}\n",
    "bbox_coords = [14.880833, 54.044444, 14.95, 54.068889]\n",
    "for year in range(2015, 2024):\n",
    "    sh_requests[year] = get_request(year, bbox_coords)\n",
    "list_of_requests = [request.download_list[0] for request in sh_requests.values()]\n",
    "data = SentinelHubDownloadClient(config=config).download(\n",
    "    list_of_requests, max_threads=5\n",
    ")\n",
    "def request_output_path(request):\n",
    "    return Path(request.data_folder, request.get_filename_list()[0])\n",
    "\n",
    "for year, request in sh_requests.items():\n",
    "    request_output_path(request).rename(f\"./data/{year}.tif\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-14T13:27:31.736743Z",
     "start_time": "2024-09-14T13:27:31.367178Z"
    }
   },
   "source": [
    "def add_time_dim(xda):\n",
    "    # This pre-processes the file to add the correct\n",
    "    # year from the filename as the time dimension\n",
    "    year = int(Path(xda.encoding[\"source\"]).stem)\n",
    "    return xda.expand_dims(year=[year])\n",
    "tiff_paths = Path(\"./data\").glob(\"*.tif\")\n",
    "ds_s2 = xr.open_mfdataset(\n",
    "    tiff_paths,\n",
    "    engine=\"rasterio\",\n",
    "    preprocess=add_time_dim,\n",
    "    band_as_variable=True,\n",
    ")\n",
    "if not ds_s2.rio.crs:\n",
    "    ds_s2 = ds_s2.rio.write_crs(\"EPSG:32633\")\n",
    "ds_s2 = ds_s2.rio.reproject(\"EPSG:4326\")\n",
    "ds_s2 = ds_s2.rename({'x': 'longitude', 'y': 'latitude'})\n",
    "# ds_s2.to_dataframe().reset_index().to_csv(\"mapped_data_leniwa_pizdo.csv\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n",
      "Warning 1: TIFFReadDirectory:Sum of Photometric type-related color channels and ExtraSamples doesn't match SamplesPerPixel. Defining non-color channels as ExtraSamples.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unrecognized chunk manager dask - must be one of: []",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m xda\u001B[38;5;241m.\u001B[39mexpand_dims(year\u001B[38;5;241m=\u001B[39m[year])\n\u001B[1;32m      6\u001B[0m tiff_paths \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./data\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mglob(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*.tif\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m----> 7\u001B[0m ds_s2 \u001B[38;5;241m=\u001B[39m \u001B[43mxr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen_mfdataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtiff_paths\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrasterio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreprocess\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_time_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m    \u001B[49m\u001B[43mband_as_variable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m ds_s2\u001B[38;5;241m.\u001B[39mrio\u001B[38;5;241m.\u001B[39mcrs:\n\u001B[1;32m     14\u001B[0m     ds_s2 \u001B[38;5;241m=\u001B[39m ds_s2\u001B[38;5;241m.\u001B[39mrio\u001B[38;5;241m.\u001B[39mwrite_crs(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEPSG:32633\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniforge3/envs/cassini/lib/python3.10/site-packages/xarray/backends/api.py:1137\u001B[0m, in \u001B[0;36mopen_mfdataset\u001B[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001B[0m\n\u001B[1;32m   1134\u001B[0m     open_ \u001B[38;5;241m=\u001B[39m open_dataset\n\u001B[1;32m   1135\u001B[0m     getattr_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m\n\u001B[0;32m-> 1137\u001B[0m datasets \u001B[38;5;241m=\u001B[39m [open_(p, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mopen_kwargs) \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m paths]\n\u001B[1;32m   1138\u001B[0m closers \u001B[38;5;241m=\u001B[39m [getattr_(ds, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_close\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m ds \u001B[38;5;129;01min\u001B[39;00m datasets]\n\u001B[1;32m   1139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m preprocess \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/cassini/lib/python3.10/site-packages/xarray/backends/api.py:1137\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m   1134\u001B[0m     open_ \u001B[38;5;241m=\u001B[39m open_dataset\n\u001B[1;32m   1135\u001B[0m     getattr_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m\n\u001B[0;32m-> 1137\u001B[0m datasets \u001B[38;5;241m=\u001B[39m [\u001B[43mopen_\u001B[49m\u001B[43m(\u001B[49m\u001B[43mp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mopen_kwargs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m paths]\n\u001B[1;32m   1138\u001B[0m closers \u001B[38;5;241m=\u001B[39m [getattr_(ds, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_close\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m ds \u001B[38;5;129;01min\u001B[39;00m datasets]\n\u001B[1;32m   1139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m preprocess \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniforge3/envs/cassini/lib/python3.10/site-packages/xarray/backends/api.py:617\u001B[0m, in \u001B[0;36mopen_dataset\u001B[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001B[0m\n\u001B[1;32m    610\u001B[0m overwrite_encoded_chunks \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite_encoded_chunks\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    611\u001B[0m backend_ds \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mopen_dataset(\n\u001B[1;32m    612\u001B[0m     filename_or_obj,\n\u001B[1;32m    613\u001B[0m     drop_variables\u001B[38;5;241m=\u001B[39mdrop_variables,\n\u001B[1;32m    614\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdecoders,\n\u001B[1;32m    615\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    616\u001B[0m )\n\u001B[0;32m--> 617\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43m_dataset_from_backend_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    618\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbackend_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    619\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfilename_or_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    620\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    621\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    622\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    623\u001B[0m \u001B[43m    \u001B[49m\u001B[43moverwrite_encoded_chunks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    624\u001B[0m \u001B[43m    \u001B[49m\u001B[43minline_array\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    625\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked_array_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfrom_array_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdrop_variables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdrop_variables\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    628\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdecoders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    629\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    630\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ds\n",
      "File \u001B[0;32m~/miniforge3/envs/cassini/lib/python3.10/site-packages/xarray/backends/api.py:393\u001B[0m, in \u001B[0;36m_dataset_from_backend_dataset\u001B[0;34m(backend_ds, filename_or_obj, engine, chunks, cache, overwrite_encoded_chunks, inline_array, chunked_array_type, from_array_kwargs, **extra_tokens)\u001B[0m\n\u001B[1;32m    391\u001B[0m     ds \u001B[38;5;241m=\u001B[39m backend_ds\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 393\u001B[0m     ds \u001B[38;5;241m=\u001B[39m \u001B[43m_chunk_ds\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbackend_ds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename_or_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m        \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m        \u001B[49m\u001B[43moverwrite_encoded_chunks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m        \u001B[49m\u001B[43minline_array\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked_array_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    401\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfrom_array_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mextra_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    405\u001B[0m ds\u001B[38;5;241m.\u001B[39mset_close(backend_ds\u001B[38;5;241m.\u001B[39m_close)\n\u001B[1;32m    407\u001B[0m \u001B[38;5;66;03m# Ensure source filename always stored in dataset object\u001B[39;00m\n",
      "File \u001B[0;32m~/miniforge3/envs/cassini/lib/python3.10/site-packages/xarray/backends/api.py:341\u001B[0m, in \u001B[0;36m_chunk_ds\u001B[0;34m(backend_ds, filename_or_obj, engine, chunks, overwrite_encoded_chunks, inline_array, chunked_array_type, from_array_kwargs, **extra_tokens)\u001B[0m\n\u001B[1;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_chunk_ds\u001B[39m(\n\u001B[1;32m    331\u001B[0m     backend_ds,\n\u001B[1;32m    332\u001B[0m     filename_or_obj,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    339\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mextra_tokens,\n\u001B[1;32m    340\u001B[0m ):\n\u001B[0;32m--> 341\u001B[0m     chunkmanager \u001B[38;5;241m=\u001B[39m \u001B[43mguess_chunkmanager\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchunked_array_type\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    343\u001B[0m     \u001B[38;5;66;03m# TODO refactor to move this dask-specific logic inside the DaskManager class\u001B[39;00m\n\u001B[1;32m    344\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunkmanager, DaskManager):\n",
      "File \u001B[0;32m~/miniforge3/envs/cassini/lib/python3.10/site-packages/xarray/namedarray/parallelcompat.py:110\u001B[0m, in \u001B[0;36mguess_chunkmanager\u001B[0;34m(manager)\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(manager, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    109\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m chunkmanagers:\n\u001B[0;32m--> 110\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    111\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munrecognized chunk manager \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmanager\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m - must be one of: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlist\u001B[39m(chunkmanagers)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    112\u001B[0m         )\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m chunkmanagers[manager]\n\u001B[1;32m    115\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(manager, ChunkManagerEntrypoint):\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;66;03m# already a valid ChunkManager so just pass through\u001B[39;00m\n",
      "\u001B[0;31mValueError\u001B[0m: unrecognized chunk manager dask - must be one of: []"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if not ds_s2.rio.crs:\n",
    "    # Assign the CRS (replace 'EPSG:32633' with your actual CRS)\n",
    "    ds_s2 = ds_s2.rio.write_crs(\"EPSG:32633\")\n",
    "ds_s2 = ds_s2.rio.reproject(\"EPSG:4326\")\n",
    "ds_s2 = ds_s2.rename({'x': 'longitude', 'y': 'latitude'})\n",
    "ds_s2.to_dataframe().reset_index().to_csv(\"mapped_data_leniwa_pizdo.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df=ds_s2.band_4.to_dataframe().reset_index()\n",
    "df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "(df['y']/df['y'].max()).min()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
